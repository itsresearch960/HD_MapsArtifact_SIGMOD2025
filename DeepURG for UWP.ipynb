{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0db8c-243d-484c-82a5-d4bb054cf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.figure()\n",
    "manager = plt.get_current_fig_manager()\n",
    "manager.full_screen_toggle()\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = [10.00, 7.00]\n",
    "\"\"\"\n",
    "Full pipeline with optional normalization:\n",
    "Load → (optional normalize) → Train → Predict → Denormalize → Evaluate → Save → Visualize\n",
    "\n",
    "Inputs\n",
    "------\n",
    "- big_tensor.npy : shape (B, T, V)  (aligned & zero-filled popularity vectors)\n",
    "\n",
    "Main Features\n",
    "-------------\n",
    "1) Concatenate first `pq` big boxes along features → (T, pq*V)\n",
    "2) Sliding windows of length `time_sequence` to predict next step\n",
    "3) Compact model: TimeDistributed(Dense)->LSTM->Dense->Dense\n",
    "4) Optional normalization (minmax or zscore), fit on TRAIN ONLY\n",
    "5) Predict on test split; denormalize for metrics/saving\n",
    "6) Save predictions/ground-truth (denormalized) and metrics\n",
    "7) Visualize:\n",
    "   - Training vs Validation loss curves\n",
    "   - Actual vs Predicted (subset of dims)\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "- allresultpfresh.npy  : (pq, N_eval, V) predictions (denormalized, original scale)\n",
    "- alltruefresh.npy     : (pq, N_eval, V) ground truth (denormalized, original scale)\n",
    "- metrics_fresh.csv    : MAE/MSE per box + overall (original scale)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Input\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "TENSOR_PATH   = \"big_UWP_tensor.npy\"\n",
    "\n",
    "pq            = 2            # number of big boxes concatenated along features\n",
    "time_sequence = 3            # sliding window length\n",
    "train_ratio   = 0.8          # chronological split\n",
    "\n",
    "# Model sizes (reduce if you hit memory limits)\n",
    "proj_dim      = 512          # per-timestep projection size\n",
    "rnn_units     = 256          # LSTM units\n",
    "learning_rate = 1e-3\n",
    "batch_size    = 32\n",
    "max_epochs    = 500\n",
    "patience      = 10\n",
    "\n",
    "# Normalization (toggle ON/OFF)\n",
    "NORMALIZE          = True     # << set False to run without normalization\n",
    "NORMALIZATION_TYPE = \"minmax\" # \"minmax\" or \"zscore\"\n",
    "\n",
    "# Saving\n",
    "SAVE_PRED     = \"predictedUWP.npy\"\n",
    "SAVE_TRUE     = \"truthUWP.npy\"\n",
    "SAVE_METRICS  = \"metrics_freshUWP.csv\"\n",
    "\n",
    "# Visualization selection\n",
    "VIS_SAMPLE_IDX = -1          # which test sample to visualize (-1 = last)\n",
    "VIS_BOX_J      = 0           # which big box slice (0..pq-1)\n",
    "VIS_K_MAX      = 2000        # first K dims to plot\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def build_feature_matrix(pvect: np.ndarray, pq: int) -> np.ndarray:\n",
    "    \"\"\"Concatenate the first pq big boxes along features: (B,T,V) -> (T, pq*V).\"\"\"\n",
    "    assert pq <= pvect.shape[0], \"pq exceeds number of big boxes in tensor\"\n",
    "    parts = [pvect[i] for i in range(pq)]              # each (T, V)\n",
    "    RCF = np.concatenate(parts, axis=1).astype(np.float32)\n",
    "    return RCF\n",
    "\n",
    "\n",
    "def make_windows(RCF: np.ndarray, time_sequence: int):\n",
    "    \"\"\"Build sliding windows: X: (N, ts, F), y: (N, F).\"\"\"\n",
    "    T, F = RCF.shape\n",
    "    X = np.array([RCF[i:i+time_sequence] for i in range(T - time_sequence)], dtype=np.float32)\n",
    "    y = np.array([RCF[i+time_sequence]    for i in range(T - time_sequence)], dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_train_test(X, y, ratio: float):\n",
    "    N = X.shape[0]\n",
    "    split = int(ratio * N)\n",
    "    print('Breakdon:',split)\n",
    "    return X[:split], y[:split], X[split:], y[split:]\n",
    "\n",
    "\n",
    "def build_model(time_sequence: int, F: int, proj_dim: int, rnn_units: int, lr: float):\n",
    "    model = Sequential([\n",
    "        Input(shape=(time_sequence, F)),\n",
    "        TimeDistributed(Dense(proj_dim, activation=\"relu\")),  # per-step compression\n",
    "        LSTM(rnn_units),\n",
    "        Dense(proj_dim, activation=\"relu\"),\n",
    "        Dense(F)                                             # back to full size\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_per_box(matrix_2d: np.ndarray, V: int):\n",
    "    \"\"\"(N, F=pq*V) -> (pq, N, V)\"\"\"\n",
    "    N, F = matrix_2d.shape\n",
    "    assert F % V == 0, \"F must be multiple of V\"\n",
    "    _pq = F // V\n",
    "    blocks = np.stack([matrix_2d[:, j*V:(j+1)*V] for j in range(_pq)], axis=0)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# ---------- Normalization helpers (fit on TRAIN ONLY) ----------\n",
    "def fit_normalizer(train_data_2d: np.ndarray, method: str):\n",
    "    \"\"\"\n",
    "    Fit normalization params on flattened train data.\n",
    "    train_data_2d: shape (N_train, F)\n",
    "    Returns params tuple depending on method.\n",
    "    \"\"\"\n",
    "    if method == \"minmax\":\n",
    "        mn = np.min(train_data_2d)\n",
    "        mx = np.max(train_data_2d)\n",
    "        return (mn, mx)\n",
    "    elif method == \"zscore\":\n",
    "        mean = np.mean(train_data_2d)\n",
    "        std  = np.std(train_data_2d) + 1e-8\n",
    "        return (mean, std)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method\")\n",
    "\n",
    "\n",
    "def apply_normalizer(data_2d: np.ndarray, params, method: str):\n",
    "    \"\"\"Apply normalization to 2D data (N, F).\"\"\"\n",
    "    if method == \"minmax\":\n",
    "        mn, mx = params\n",
    "        return (data_2d - mn) / (mx - mn + 1e-8)\n",
    "    elif method == \"zscore\":\n",
    "        mean, std = params\n",
    "        return (data_2d - mean) / std\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method\")\n",
    "\n",
    "\n",
    "def invert_normalizer(data_2d: np.ndarray, params, method: str):\n",
    "    \"\"\"Invert normalization to original scale for 2D data (N, F).\"\"\"\n",
    "    if method == \"minmax\":\n",
    "        mn, mx = params\n",
    "        return data_2d * (mx - mn + 1e-8) + mn\n",
    "    elif method == \"zscore\":\n",
    "        mean, std = params\n",
    "        return data_2d * std + mean\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method\")\n",
    "\n",
    "\n",
    "# ---------------- Visualization ----------------\n",
    "def visualize_loss(history):\n",
    "    n = len(history.history.get(\"loss\", []))\n",
    "    m = min(20, n) if n > 0 else 0\n",
    "    if m == 0:\n",
    "        print(\"No loss history to plot.\")\n",
    "        return\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'][:m], c='r', label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'][:m], c='g', label='Validation Loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.title(f\"Training vs Validation Loss (first {m} epochs)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_vectors(true_per_box, pred_per_box, box_j, sample_idx, k_max):\n",
    "    V = true_per_box.shape[2]\n",
    "    if true_per_box.shape[1] == 0:\n",
    "        print(\"No samples to visualize.\")\n",
    "        return\n",
    "    # normalize negative sample index\n",
    "    si = sample_idx if sample_idx >= 0 else (true_per_box.shape[1] + sample_idx)\n",
    "    si = max(0, min(si, true_per_box.shape[1] - 1))\n",
    "    K = min(k_max, V)\n",
    "\n",
    "    y_true_vec = true_per_box[box_j, si]\n",
    "    y_pred_vec = pred_per_box[box_j, si]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(y_true_vec[:K], label=\"true\", linewidth=1)\n",
    "    plt.plot(y_pred_vec[:K], label=\"pred\", linewidth=1)\n",
    "    plt.title(f\"Actual vs Predicted (Box {box_j}, sample {si})\")\n",
    "    plt.xlabel(f\"small_id (first {K})\")\n",
    "    plt.ylabel(\"popularity count\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1) Load tensor\n",
    "pvect = np.load(TENSOR_PATH)  # (B, T, V)\n",
    "B, T_total, V = pvect.shape\n",
    "print(f\"Loaded tensor: {pvect.shape} (B,T,V)\")\n",
    "\n",
    "# 2) Features\n",
    "RCF = build_feature_matrix(pvect, pq)    # (T, pq*V)\n",
    "T, F = RCF.shape\n",
    "print(f\"Concatenated: (T={T}, F={F}) with pq={pq}, V={V}\")\n",
    "\n",
    "# 3) Windows\n",
    "X, y = make_windows(RCF, time_sequence)  # X: (N, ts, F), y: (N, F)\n",
    "N = X.shape[0]\n",
    "print(f\"Windows: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# 4) Split\n",
    "x_train, y_train, x_test, y_test = split_train_test(X, y, train_ratio)\n",
    "print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")\n",
    "\n",
    "# 5) Optional normalization (fit on TRAIN ONLY)\n",
    "if NORMALIZE:\n",
    "    # Fit on train y (targets) and train X (inputs) using flattened 2D views\n",
    "    y_train_2d = y_train.reshape(y_train.shape[0], -1)  # (N_train, F)\n",
    "    params_y = fit_normalizer(y_train_2d, NORMALIZATION_TYPE)\n",
    "\n",
    "    # Normalize y\n",
    "    y_train = apply_normalizer(y_train_2d, params_y, NORMALIZATION_TYPE).reshape(y_train.shape)\n",
    "    y_test  = apply_normalizer(y_test.reshape(y_test.shape[0], -1), params_y, NORMALIZATION_TYPE).reshape(y_test.shape)\n",
    "\n",
    "    # Normalize X per feature as well using SAME params_y (simple, consistent)\n",
    "    # (You could fit separate params for X; here we use y's params to keep scale aligned.)\n",
    "    x_train = apply_normalizer(x_train.reshape(x_train.shape[0], -1), params_y, NORMALIZATION_TYPE).reshape(x_train.shape)\n",
    "    x_test  = apply_normalizer(x_test.reshape(x_test.shape[0], -1), params_y, NORMALIZATION_TYPE).reshape(x_test.shape)\n",
    "\n",
    "    print(f\"Normalization ON ({NORMALIZATION_TYPE}). Params learned on TRAIN only.\")\n",
    "else:\n",
    "    print(\"Normalization OFF.\")\n",
    "\n",
    "# 6) Model\n",
    "model = build_model(time_sequence, F, proj_dim, rnn_units, learning_rate)\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7) Loss curve\n",
    "visualize_loss(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4520a8-d080-4e98-a371-25227bf5545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Split\n",
    "x_train, y_train, x_test, y_test = split_train_test(X, y, 0.0)\n",
    "print(f\"Train: {x_train.shape}, Test: {x_test.shape}\")\n",
    "\n",
    "# 8) Predict on test\n",
    "y_pred = model.predict(x_test, verbose=0)  # (N_eval, F)\n",
    "N_eval = y_pred.shape[0]\n",
    "print(f\"Predicted: {y_pred.shape}\")\n",
    "\n",
    "# 9) Denormalize predictions & ground-truth back to original scale for metrics/saving\n",
    "if NORMALIZE:\n",
    "    y_pred = invert_normalizer(y_pred, params_y, NORMALIZATION_TYPE)\n",
    "    y_test = invert_normalizer(y_test, params_y, NORMALIZATION_TYPE)\n",
    "\n",
    "# 10) Split per box (pq, N_eval, V)\n",
    "pred_per_box = split_per_box(y_pred, V)\n",
    "true_per_box = split_per_box(y_test, V)\n",
    "\n",
    "# 11) Metrics (original scale)\n",
    "mae_per_box = np.mean(np.abs(pred_per_box - true_per_box), axis=(1,2))\n",
    "mse_per_box = np.mean((pred_per_box - true_per_box) ** 2, axis=(1,2))\n",
    "mae_overall = np.mean(np.abs(y_pred - y_test))\n",
    "mse_overall = np.mean((y_pred - y_test) ** 2)\n",
    "\n",
    "print(\"\\n=== Evaluation (Original Scale) ===\")\n",
    "for j in range(pq):\n",
    "    print(f\"Box {j}: MAE={mae_per_box[j]:.6f}  MSE={mse_per_box[j]:.6f}\")\n",
    "print(f\"Overall: MAE={mae_overall:.6f}  MSE={mse_overall:.6f}\")\n",
    "\n",
    "# 12) Save outputs\n",
    "os.makedirs(os.path.dirname(SAVE_PRED), exist_ok=True)\n",
    "np.save(SAVE_PRED, pred_per_box)  # (pq, N_eval, V)\n",
    "np.save(SAVE_TRUE, true_per_box)  # (pq, N_eval, V)\n",
    "with open(SAVE_METRICS, \"w\") as f:\n",
    "    f.write(\"box,mae,mse\\n\")\n",
    "    for j in range(pq):\n",
    "        f.write(f\"{j},{mae_per_box[j]},{mse_per_box[j]}\\n\")\n",
    "    f.write(f\"overall,{mae_overall},{mse_overall}\\n\")\n",
    "\n",
    "print(f\"\\nSaved predictions: {SAVE_PRED}  shape={pred_per_box.shape}\")\n",
    "print(f\"Saved ground truth: {SAVE_TRUE}  shape={true_per_box.shape}\")\n",
    "print(f\"Saved metrics CSV: {SAVE_METRICS}\")\n",
    "\n",
    "# 13) Visualization: Actual vs Predicted (subset of dims)\n",
    "if N_eval > 0:\n",
    "    visualize_vectors(true_per_box, pred_per_box,\n",
    "                      box_j=VIS_BOX_J, sample_idx=VIS_SAMPLE_IDX, k_max=VIS_K_MAX)\n",
    "else:\n",
    "    print(\"No evaluation samples to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c48943-1dd8-4565-bf84-b19d0ed0a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Construct per–cellular-area (big box) map-segment request sequences, one per timeslot, \n",
    "using a sliding window of the most recent `time_sequence` steps. Because the downstream model\n",
    "is LSTM-based, both inputs and targets are temporally aligned: for each index t, the input window \n",
    "aggregates the last `time_sequence` slots [t−time_sequence, …, t−1], and the corresponding map-request data \n",
    "(targets) is taken at the next slot t. This ensures the training data reflects causal, ordered context consistent \n",
    "with UWP-driven demand dynamics.\n",
    "'''\n",
    "car4=np.load('car4real.npy',allow_pickle=True)[0]\n",
    "Timeslot=list(car4[21].keys())\n",
    "CELLULAR=[21,22]\n",
    "MP=[]\n",
    "for q in CELLULAR:\n",
    "    car10=car4[q]\n",
    "    carr10=[]\n",
    "    for i in range(149):\n",
    "        carr10.append(car10[Timeslot[i]])\n",
    "    for el in carr10:\n",
    "        el.sort(key=lambda x: x[1])\n",
    "    mapsegment=[]\n",
    "    for p in range(len(carr10)):\n",
    "        cachep={}\n",
    "        for i in range(1,7):\n",
    "            cachep[i]=[]\n",
    "        for slot in carr10[p]:\n",
    "            for el in slot[2]:\n",
    "                cachep[int(slot[1]/10)+1].append([(el[0]),(el[1])])\n",
    "        mapsegment.append(cachep)\n",
    "    MP.append(mapsegment)\n",
    "mpti=[]\n",
    "mpto=[]\n",
    "for el in MP:\n",
    "    mpti.append([[el[i+j] for j in range(time_sequence)] for i in range(len(el)-time_sequence)])\n",
    "    mpto.append([[el[i+time_sequence]] for i in range(len(el)-time_sequence)]) \n",
    "\n",
    "np.save('map_requests.npy',mpto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
