{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c3081-17fd-4842-96c7-a8f61cf7e669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR  = \"tdrive_boxes_time_MINUTES\"      # where *_boxes_reach.csv live\n",
    "OUTPUT_DIR = \"tdrive_popularity_sparse_1hour_250\" # one file per starting big box\n",
    "\n",
    "# Expected columns in input\n",
    "COL_FILE       = \"file\"\n",
    "COL_DATE       = \"date\"\n",
    "COL_SLOT       = \"slot_id\"\n",
    "COL_BOX_LABELS = \"box_labels\"   # \"big/small/time;big/small/time;...\"\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def parse_label_loose(token: str):\n",
    "    \"\"\"\n",
    "    Parse a token like 'big/small/time' or 'big/small' into (big_id:int, small_id:int).\n",
    "    We deliberately ignore time; we only need big and small.\n",
    "    Returns None if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = token.strip().split(\"/\")\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        big_id = int(parts[0].strip())\n",
    "        small_id = int(parts[1].strip())\n",
    "        return big_id, small_id\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_day_index_map(all_dates):\n",
    "    \"\"\"\n",
    "    Map unique date strings -> day_idx (1..N) in ascending date order.\n",
    "    \"\"\"\n",
    "    unique_sorted = sorted(set(all_dates))\n",
    "    return {d: i + 1 for i, d in enumerate(unique_sorted)}\n",
    "\n",
    "\n",
    "def process_folder(input_dir: str, output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Load all *_boxes_reach.csv\n",
    "    paths = sorted(glob.glob(os.path.join(input_dir, \"*_boxes_reach.csv\")))\n",
    "    if not paths:\n",
    "        print(\"No *_boxes_reach.csv files found.\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p, dtype={COL_FILE: str, COL_DATE: str}, keep_default_na=False)\n",
    "            cols = [COL_FILE, COL_DATE, COL_SLOT, COL_BOX_LABELS]\n",
    "            df = df[[c for c in cols if c in df.columns]].copy()\n",
    "            frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {os.path.basename(p)}: {e}\")\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No valid data after reading files.\")\n",
    "        return\n",
    "\n",
    "    data = pd.concat(frames, ignore_index=True)\n",
    "    if data.empty:\n",
    "        print(\"Input data is empty.\")\n",
    "        return\n",
    "\n",
    "    # 2) Build day indices\n",
    "    day_map = build_day_index_map(data[COL_DATE].tolist())\n",
    "    data[\"day_idx\"] = data[COL_DATE].map(day_map)\n",
    "\n",
    "    # 3) Accumulator:\n",
    "    #    counts[start_big][(day_idx, slot_id)] = Counter({small_id: crossings})\n",
    "    counts = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "    # 4) Iterate rows; count EVERY crossing (revisits included)\n",
    "    for _, row in data.iterrows():\n",
    "        date_str   = row.get(COL_DATE, \"\")\n",
    "        slot_val   = row.get(COL_SLOT, None)\n",
    "        labels_str = row.get(COL_BOX_LABELS, \"\")\n",
    "\n",
    "        if not date_str or slot_val is None or not labels_str:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            slot_id = int(slot_val)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        tokens = [tok.strip() for tok in labels_str.split(\";\") if tok.strip()]\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        # Starting big box from the first label\n",
    "        first = parse_label_loose(tokens[0])\n",
    "        if first is None:\n",
    "            continue\n",
    "        start_big, _ = first\n",
    "\n",
    "        day_idx = day_map[date_str]\n",
    "        ctr = counts[start_big][(day_idx, slot_id)]\n",
    "\n",
    "        # Count every occurrence of small_id (revisits included)\n",
    "        for tok in tokens:\n",
    "            parsed = parse_label_loose(tok)\n",
    "            if parsed is None:\n",
    "                continue\n",
    "            _, small_id = parsed\n",
    "            ctr[int(small_id)] += 1\n",
    "\n",
    "    # 5) Write one CSV per starting big box\n",
    "    for start_big, bucket in counts.items():\n",
    "        out_path = os.path.join(output_dir, f\"big_{start_big}_popularity_sparse.csv\")\n",
    "        with open(out_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Header: day_idx, slot_id, nnz (#unique small boxes), total_crossings, counts_json\n",
    "            writer.writerow([\"day_idx\", \"slot_id\", \"nnz\", \"total_crossings\", \"counts_json\"])\n",
    "            for (day_idx, slot_id), ctr in sorted(bucket.items()):\n",
    "                nnz = len(ctr)\n",
    "                total_crossings = int(sum(ctr.values()))\n",
    "                # JSON object: keys as strings for strict JSON; values as ints\n",
    "                counts_json = json.dumps({str(k): int(v) for k, v in sorted(ctr.items())},\n",
    "                                         separators=(',', ':'), sort_keys=True)\n",
    "                writer.writerow([day_idx, slot_id, nnz, total_crossings, counts_json])\n",
    "\n",
    "        print(f\"[OK] Wrote {os.path.basename(out_path)} with {len(bucket)} rows\")\n",
    "\n",
    "    # Also write day index map for reference\n",
    "    pd.DataFrame(\n",
    "        [{\"date\": d, \"day_idx\": idx} for d, idx in sorted(day_map.items(), key=lambda x: x[1])]\n",
    "    ).to_csv(os.path.join(output_dir, \"day_index_map.csv\"), index=False)\n",
    "    print(\"[OK] Wrote day_index_map.csv\")\n",
    "def main():\n",
    "    process_folder(INPUT_DIR, OUTPUT_DIR)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d08c8b-2602-46b9-982b-ba25a1217b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chosing the cellular areas (big_boxes) for the experimemts.\n",
    "Enumerate small_ids across selected big boxes, align rows/columns, and write:\n",
    "  - global_small_ids.csv        : col_idx (0-based), small_id\n",
    "  - reverse_small_ids.csv       : small_id, col_idx (0-based)\n",
    "  - big_<ID>_vectors.csv        : day_idx, slot_id, vector (zero-filled, aligned)\n",
    "\n",
    "Behavior:\n",
    "- Columns = UNION of small_ids observed in the SELECTED files only (not the whole folder).\n",
    "- Rows    = UNION of (day_idx, slot_id) across the SELECTED files; zero-fill where missing.\n",
    "- Values  = parsed from counts_json as float (works for both popularity ints and UWP floats).\n",
    "\n",
    "Adjust SPARSE_DIR / OUT_DIR / BIG_IDS as needed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "SPARSE_DIR = \"tdrive_popularity_sparse_1hour_250\"  # or popularity folder\n",
    "OUT_DIR    = \"Popularity_ENUM_Minutes\"    # output folder\n",
    "FILE_GLOB  = \"big_*_popularity_sparse.csv\"\n",
    "\n",
    "# Select ONLY these big boxes; global small_ids are built from them\n",
    "BIG_IDS = [21, 22]   # <-- edit as needed Change according to need, if want to reproduce result for Tdrive then the is [21,22], \n",
    "#for San Francisco [12,13]\n",
    "\n",
    "\n",
    "# Vector serialization (keep float to support UWP). If using pure ints, it's fine too.\n",
    "ROUND_DECIMALS = 6\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def safe_load_json(s: str):\n",
    "    try:\n",
    "        return json.loads(s) if isinstance(s, str) and s else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def list_available_files(sparse_dir: str, pattern: str):\n",
    "    \"\"\"Return {big_id: filepath} for all matching files.\"\"\"\n",
    "    out = {}\n",
    "    for p in glob.glob(os.path.join(sparse_dir, pattern)):\n",
    "        base = os.path.basename(p)\n",
    "        if base.startswith(\"big_\") and base.endswith(\"_popularity_sparse.csv\"):\n",
    "            core = base[len(\"big_\"):-len(\"_popularity_sparse.csv\")]\n",
    "            try:\n",
    "                out[int(core)] = p\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def collect_union_small_ids(files_map):\n",
    "    \"\"\"Union of small_ids across the selected files, returned as a sorted list.\"\"\"\n",
    "    sids = set()\n",
    "    for _, path in sorted(files_map.items()):\n",
    "        try:\n",
    "            df = pd.read_csv(path, usecols=[\"counts_json\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "        for _, r in df.iterrows():\n",
    "            d = safe_load_json(r.get(\"counts_json\", \"\"))\n",
    "            for k in d.keys():\n",
    "                try:\n",
    "                    sids.add(int(k))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return sorted(sids)\n",
    "\n",
    "\n",
    "def collect_union_day_slot(files_map):\n",
    "    \"\"\"Union of (day_idx, slot_id) across selected files, returned as a sorted DataFrame.\"\"\"\n",
    "    pairs = set()\n",
    "    for _, path in files_map.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, usecols=[\"day_idx\", \"slot_id\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "        for _, r in df.iterrows():\n",
    "            d = r.get(\"day_idx\")\n",
    "            s = r.get(\"slot_id\")\n",
    "            if pd.notna(d) and pd.notna(s):\n",
    "                try:\n",
    "                    pairs.add((int(d), int(s)))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not pairs:\n",
    "        return pd.DataFrame(columns=[\"day_idx\", \"slot_id\"])\n",
    "    return pd.DataFrame(sorted(pairs), columns=[\"day_idx\", \"slot_id\"])\n",
    "\n",
    "\n",
    "def write_global_index(out_dir, small_ids_sorted):\n",
    "    \"\"\"Write the enumeration mapping: col_idx (0-based), small_id.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, \"global_small_ids.csv\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"col_idx\", \"small_id\"])\n",
    "        for i, sid in enumerate(small_ids_sorted):  # 0-based\n",
    "            w.writerow([i, sid])\n",
    "    print(f\"[OK] Wrote global_small_ids.csv (K={len(small_ids_sorted)})\")\n",
    "\n",
    "\n",
    "def write_reverse_mapping(out_dir, small_ids_sorted):\n",
    "    \"\"\"Write reverse mapping: small_id -> col_idx (0-based).\"\"\"\n",
    "    path = os.path.join(out_dir, \"reverse_small_ids.csv\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"small_id\", \"col_idx\"])\n",
    "        for i, sid in enumerate(small_ids_sorted):  # 0-based\n",
    "            w.writerow([sid, i])\n",
    "    print(f\"[OK] Wrote reverse_small_ids.csv (K={len(small_ids_sorted)})\")\n",
    "\n",
    "\n",
    "def vector_to_string(vec):\n",
    "    return \"[\" + \",\".join(f\"{v:.{ROUND_DECIMALS}f}\" for v in vec) + \"]\"\n",
    "\n",
    "\n",
    "def load_sparse_map(path):\n",
    "    \"\"\"\n",
    "    Load one big_* file into:\n",
    "      {(day_idx, slot_id): {small_id: float_value, ...}}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return out\n",
    "\n",
    "    if not {\"day_idx\", \"slot_id\", \"counts_json\"}.issubset(df.columns):\n",
    "        return out\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        d = r.get(\"day_idx\")\n",
    "        s = r.get(\"slot_id\")\n",
    "        if pd.isna(d) or pd.isna(s):\n",
    "            continue\n",
    "        try:\n",
    "            key = (int(d), int(s))\n",
    "        except Exception:\n",
    "            continue\n",
    "        dd = {}\n",
    "        counts = safe_load_json(r.get(\"counts_json\", \"\"))\n",
    "        for k, v in counts.items():\n",
    "            try:\n",
    "                dd[int(k)] = float(v)  # supports UWP floats & int popularity\n",
    "            except Exception:\n",
    "                continue\n",
    "        out[key] = dd\n",
    "    return out\n",
    "\n",
    "\n",
    "def convert_to_vectors(files_map, small_ids_sorted, union_day_slot, out_dir):\n",
    "    \"\"\"Write one vectors CSV per selected big box, zero-filling missing.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    col_index = {sid: i for i, sid in enumerate(small_ids_sorted)}\n",
    "    K = len(small_ids_sorted)\n",
    "\n",
    "    for bid, path in sorted(files_map.items()):\n",
    "        sparse_map = load_sparse_map(path)\n",
    "        rows = []\n",
    "\n",
    "        for _, rs in union_day_slot.iterrows():\n",
    "            d = int(rs[\"day_idx\"])\n",
    "            s = int(rs[\"slot_id\"])\n",
    "            dd = sparse_map.get((d, s), {})  # {} => zero vector\n",
    "\n",
    "            vec = [0.0] * K\n",
    "            for sid, val in dd.items():\n",
    "                j = col_index.get(sid)\n",
    "                if j is not None:\n",
    "                    vec[j] = val\n",
    "\n",
    "            rows.append({\"day_idx\": d, \"slot_id\": s, \"vector\": vector_to_string(vec)})\n",
    "\n",
    "        out_df = pd.DataFrame(rows).sort_values([\"day_idx\", \"slot_id\"])\n",
    "        out_name = f\"big_{bid}_vectors.csv\"\n",
    "        out_path = os.path.join(out_dir, out_name)\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"[OK] Wrote {out_name}  rows={len(out_df)}  vec_dim={K}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Keep ONLY the selected big boxes\n",
    "    all_files = list_available_files(SPARSE_DIR, FILE_GLOB)\n",
    "    files_map = {bid: all_files[bid] for bid in BIG_IDS if bid in all_files}\n",
    "    missing = [bid for bid in BIG_IDS if bid not in files_map]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Missing big IDs: {missing}\")\n",
    "    if not files_map:\n",
    "        print(\"ERROR: none of the requested big IDs exist in the folder.\")\n",
    "        return\n",
    "\n",
    "    # 1) UNION of small_ids across selected files -> enumerate 0..K-1\n",
    "    small_ids_sorted = collect_union_small_ids(files_map)\n",
    "    write_global_index(OUT_DIR, small_ids_sorted)\n",
    "    write_reverse_mapping(OUT_DIR, small_ids_sorted)\n",
    "\n",
    "    # 2) UNION of (day_idx, slot_id) across selected files -> consistent rows\n",
    "    union_day_slot = collect_union_day_slot(files_map)\n",
    "    if union_day_slot.empty:\n",
    "        print(\"[INFO] No (day_idx, slot_id) pairs found. Nothing to write.\")\n",
    "        return\n",
    "\n",
    "    # 3) Convert each selected file -> aligned, zero-filled vectors\n",
    "    convert_to_vectors(files_map, small_ids_sorted, union_day_slot, OUT_DIR)\n",
    "\n",
    "    print(f\"All done. Outputs at: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57282862-255e-4c9b-a6e8-aa17a9363e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR = \"Popularity_ENUM_Minutes\"      # Folder where your CSVs are\n",
    "FILES = [\"big_21_vectors.csv\", \"big_22_vectors.csv\"]  # Add more if needed\n",
    "# <-- edit as needed Change according to need, if want to reproduce result for Tdrive then the is [21,22], \n",
    "#for San Francisco [12,13]\n",
    "SAVE_PATH = \"big_Popularity_tensor.npy\"        # Save in current directory\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def parse_vector(vec_str):\n",
    "    \"\"\"Convert '[0,1,2,...]' string into list[float].\"\"\"\n",
    "    try:\n",
    "        return [float(x) for x in ast.literal_eval(vec_str)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "tensors = []\n",
    "\n",
    "for fname in FILES:\n",
    "    path = os.path.join(INPUT_DIR, fname)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] File not found: {path}\")\n",
    "        continue\n",
    "\n",
    "    # Read CSV and parse vector column\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"vector_list\"] = df[\"vector\"].apply(parse_vector)\n",
    "\n",
    "    # Convert to NumPy array (num_timeslots, vector_len)\n",
    "    arr = np.array(df[\"vector_list\"].to_list())\n",
    "    tensors.append(arr)\n",
    "    print(f\"[OK] Loaded {fname} → shape {arr.shape}\")\n",
    "\n",
    "if not tensors:\n",
    "    raise RuntimeError(\"No valid CSV files loaded. Exiting.\")\n",
    "\n",
    "# Stack all arrays into one 3D tensor: (num_big_boxes, num_timeslots, vector_len)\n",
    "tensor = np.stack(tensors, axis=0)\n",
    "print(f\"\\n Final tensor shape: {tensor.shape}\")\n",
    "\n",
    "# Save tensor in the **current directory** as `big_tensor.npy`\n",
    "np.save(SAVE_PATH, tensor)\n",
    "print(f\"[OK] Saved combined tensor → {os.path.abspath(SAVE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bd5a1-b4d8-4863-a66a-537b09bbe9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
