{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c3081-17fd-4842-96c7-a8f61cf7e669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "T-Drive: timeslot routing reconstruction (bbox + cache, OSMnx 2.0.6)\n",
    "+ average speed per timeslot\n",
    "+ minute-within-slot metadata\n",
    "\n",
    "- Reads all taxi .txt files from INPUT_DIR (format: taxi_id,timestamp,lon,lat).\n",
    "- Buckets records into M_MINUTES slots starting at 00:00 (UTC).\n",
    "- For each (date, slot), sorts by time, snaps points to nearest OSM node,\n",
    "  computes shortest paths between consecutive points (NetworkX), and sums distances.\n",
    "- Adds:\n",
    "  - slot_size_min           : slot length in minutes (M_MINUTES)\n",
    "  - slot_start_utc          : UTC timestamp of the slot’s start (date 00:00 + slot*M_MINUTES)\n",
    "  - start_minute_in_slot    : minute offset (0..M_MINUTES-1) of the first point within the slot\n",
    "  - start_offset_s          : exact seconds offset from slot start to first point\n",
    "- Writes:\n",
    "  - <output_dir>/<file>_slots.csv : per-slot summary\n",
    "  - (optional) <output_dir>/<file>_paths.geojson : reconstructed polylines\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "from typing import Tuple, List, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tqdm is optional\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x): return x\n",
    "\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, box\n",
    "\n",
    "# -------------------------- USER CONFIG ---------------------------------------\n",
    "\n",
    "INPUT_DIR  = \"tdrive\"\n",
    "OUTPUT_DIR = \"tdrive_routes_speed_MINUTES\"\n",
    "GRAPH_CACHE_DIR = OUTPUT_DIR\n",
    "\n",
    "M_MINUTES = 60  # slot size in minutes\n",
    "\n",
    "# Use a bbox to avoid full-city queries: (min_lat, max_lat, min_lon, max_lon)\n",
    "BBOX = (39.8, 40.05, 116.2, 116.5) #For TDRIVE DATASET\n",
    "#BBOX = (37.60002136230469, 37.75, -122.39936828613281, -122.25) # For San Francisco Dataset\n",
    "\n",
    "# Weight for shortest path: \"length\" (meters) or \"travel_time\" (if you add speeds)\n",
    "WEIGHT = \"length\"\n",
    "\n",
    "# Export polylines for each slot (disable for speed while testing)\n",
    "WRITE_GEOJSON = True\n",
    "\n",
    "# Skip routing if consecutive point time gap exceeds this (minutes)\n",
    "MAX_GAP_MIN = 60  # set None to disable\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _graph_cache_path(bbox: Tuple[float, float, float, float]) -> str:\n",
    "    miny, maxy, minx, maxx = bbox\n",
    "    tag = f\"{miny:.4f}_{maxy:.4f}_{minx:.4f}_{maxx:.4f}\"\n",
    "    return os.path.join(GRAPH_CACHE_DIR, f\"drive_bbox_{tag}.graphml\")\n",
    "\n",
    "\n",
    "def load_graph() -> nx.MultiDiGraph:\n",
    "    \"\"\"\n",
    "    Load drivable OSM graph for the configured BBOX, using local cache.\n",
    "    Compatible with OSMnx 2.0.6 (polygon-based).\n",
    "    \"\"\"\n",
    "    os.makedirs(GRAPH_CACHE_DIR, exist_ok=True)\n",
    "    ox.settings.use_cache = True\n",
    "    ox.settings.log_console = False\n",
    "\n",
    "    miny, maxy, minx, maxx = BBOX\n",
    "    cache_path = _graph_cache_path(BBOX)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        G = ox.load_graphml(cache_path)\n",
    "    else:\n",
    "        polygon = box(minx, miny, maxx, maxy)\n",
    "        G = ox.graph_from_polygon(polygon, network_type=\"drive\")\n",
    "        G = ox.distance.add_edge_lengths(G)  # adds 'length' to edges\n",
    "        ox.save_graphml(G, cache_path)\n",
    "\n",
    "    # Ensure 'length' exists (paranoia)\n",
    "    if not all('length' in d for _, _, d in G.edges(data=True)):\n",
    "        G = ox.distance.add_edge_lengths(G)\n",
    "        ox.save_graphml(G, cache_path)\n",
    "    return G\n",
    "\n",
    "\n",
    "def read_tdrive_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read T-Drive txt: taxi_id,timestamp,lon,lat (no header).\n",
    "    Adds: date (YYYY-MM-DD string), slot_id (00:00-based index).\n",
    "    All timestamps are parsed as UTC.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=[\"taxi_id\", \"timestamp\", \"lon\", \"lat\"],\n",
    "        dtype={\"taxi_id\": str, \"timestamp\": str, \"lon\": float, \"lat\": float},\n",
    "    )\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "    df = df.dropna(subset=[\"timestamp\", \"lon\", \"lat\"]).copy()\n",
    "\n",
    "    df[\"date\"] = df[\"timestamp\"].dt.date.astype(str)\n",
    "    minutes = df[\"timestamp\"].dt.hour * 60 + df[\"timestamp\"].dt.minute\n",
    "    df[\"slot_id\"] = (minutes // M_MINUTES).astype(int)\n",
    "    return df.sort_values(\"timestamp\")\n",
    "\n",
    "\n",
    "def snap_points_to_nodes(G: nx.MultiDiGraph, df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Snap each (lon,lat) to the nearest OSM node. Returns node ids (np.array).\n",
    "    \"\"\"\n",
    "    xs = df[\"lon\"].to_numpy()\n",
    "    ys = df[\"lat\"].to_numpy()\n",
    "    nodes = ox.distance.nearest_nodes(G, X=xs, Y=ys)\n",
    "    return np.array(nodes)\n",
    "\n",
    "\n",
    "def path_between_nodes(G: nx.MultiDiGraph, u: int, v: int, weight: str = \"length\") -> Optional[List[int]]:\n",
    "    \"\"\"\n",
    "    Shortest path node-list between u and v using NetworkX. None if no path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if u == v:\n",
    "            return [u]\n",
    "        return nx.shortest_path(G, u, v, weight=weight)\n",
    "    except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "        return None\n",
    "\n",
    "\n",
    "def path_length_m(G: nx.MultiDiGraph, path: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Sum edge lengths (meters) along a node path.\n",
    "    \"\"\"\n",
    "    if not path or len(path) < 2:\n",
    "        return 0.0\n",
    "    total = 0.0\n",
    "    for a, b in zip(path[:-1], path[1:]):\n",
    "        if G.has_edge(a, b):\n",
    "            edges = G.get_edge_data(a, b)\n",
    "            if not edges:\n",
    "                continue\n",
    "            # choose the minimum length among parallel edges\n",
    "            best = min((edata.get(\"length\", math.inf) for _, edata in edges.items()))\n",
    "            if math.isfinite(best):\n",
    "                total += best\n",
    "    return total\n",
    "\n",
    "\n",
    "def build_linestring_from_path(G: nx.MultiDiGraph, path: List[int]) -> Optional[LineString]:\n",
    "    \"\"\"\n",
    "    Convert a node path into a LineString in EPSG:4326.\n",
    "    \"\"\"\n",
    "    if not path:\n",
    "        return None\n",
    "    coords = []\n",
    "    for n in path:\n",
    "        nd = G.nodes[n]\n",
    "        x, y = nd.get(\"x\"), nd.get(\"y\")\n",
    "        if x is None or y is None:\n",
    "            return None\n",
    "        coords.append((x, y))\n",
    "    if len(coords) < 2:\n",
    "        return None\n",
    "    return LineString(coords)\n",
    "\n",
    "\n",
    "def process_file(G: nx.MultiDiGraph, infile: str, outdir: str) -> None:\n",
    "    \"\"\"\n",
    "    Process a single taxi file and write CSV (+ optional GeoJSON).\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(infile)\n",
    "    stem = os.path.splitext(basename)[0]\n",
    "\n",
    "    df = read_tdrive_file(infile)\n",
    "    if df.empty:\n",
    "        print(f\"[SKIP] {basename}: no valid rows\")\n",
    "        return\n",
    "\n",
    "    df[\"node\"] = snap_points_to_nodes(G, df)\n",
    "\n",
    "    rows = []\n",
    "    geoms = []\n",
    "    gprops = []\n",
    "\n",
    "    for (date, slot), g in df.groupby([\"date\", \"slot_id\"], sort=True):\n",
    "        g = g.sort_values(\"timestamp\")\n",
    "        npts = len(g)\n",
    "\n",
    "        # Compute slot start instant in UTC (date 00:00 + slot*M_MINUTES)\n",
    "        slot_start_utc = pd.to_datetime(f\"{date} 00:00:00\", utc=True)\n",
    "        slot_start_utc = slot_start_utc + pd.Timedelta(minutes=int(slot) * M_MINUTES)\n",
    "\n",
    "        # Record start/end times for duration\n",
    "        start_time = g[\"timestamp\"].iloc[0]\n",
    "        end_time   = g[\"timestamp\"].iloc[-1]\n",
    "        duration_s = max(0.0, (end_time - start_time).total_seconds())\n",
    "        duration_min = duration_s / 60.0\n",
    "\n",
    "        # Offsets relative to the slot start\n",
    "        start_offset_s = max(0.0, (start_time - slot_start_utc).total_seconds())\n",
    "        start_minute_in_slot = int(start_offset_s // 60)  # 0 .. M_MINUTES-1\n",
    "\n",
    "        if npts < 2:\n",
    "            rows.append({\n",
    "                \"file\": basename,\n",
    "                \"date\": date,\n",
    "                \"slot_id\": slot,\n",
    "                \"slot_size_min\": M_MINUTES,\n",
    "                \"slot_start_utc\": slot_start_utc,\n",
    "                \"start_minute_in_slot\": start_minute_in_slot,\n",
    "                \"start_offset_s\": start_offset_s,\n",
    "                \"n_points\": npts,\n",
    "                \"n_paths\": 0,\n",
    "                \"total_length_m\": 0.0,\n",
    "                \"duration_s\": duration_s,\n",
    "                \"duration_min\": duration_min,\n",
    "                \"avg_speed_kmh\": 0.0,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        times = g[\"timestamp\"].to_numpy()\n",
    "        nodes = g[\"node\"].to_numpy()\n",
    "\n",
    "        slot_paths: List[List[int]] = []\n",
    "        slot_length = 0.0\n",
    "        n_paths = 0\n",
    "\n",
    "        for i in range(npts - 1):\n",
    "            t0 = pd.Timestamp(times[i])\n",
    "            t1 = pd.Timestamp(times[i + 1])\n",
    "            if MAX_GAP_MIN is not None:\n",
    "                gap_min = (t1 - t0).total_seconds() / 60.0\n",
    "                if gap_min > MAX_GAP_MIN:\n",
    "                    continue  # skip unrealistic jump\n",
    "\n",
    "            u = int(nodes[i])\n",
    "            v = int(nodes[i + 1])\n",
    "            if u == v:\n",
    "                continue  # no movement\n",
    "\n",
    "            path = path_between_nodes(G, u, v, weight=WEIGHT)\n",
    "            if path is None:\n",
    "                continue\n",
    "\n",
    "            slot_paths.append(path)\n",
    "            slot_length += path_length_m(G, path)\n",
    "            n_paths += 1\n",
    "\n",
    "        # Average speed (km/h) over the slot, using total_length_m and slot duration\n",
    "        if duration_s > 0:\n",
    "            avg_speed_kmh = (slot_length / 1000.0) / (duration_s / 3600.0)\n",
    "        else:\n",
    "            avg_speed_kmh = 0.0\n",
    "\n",
    "        rows.append({\n",
    "            \"file\": basename,\n",
    "            \"date\": date,\n",
    "            \"slot_id\": slot,\n",
    "            \"slot_size_min\": M_MINUTES,\n",
    "            \"slot_start_utc\": slot_start_utc,\n",
    "            \"start_minute_in_slot\": start_minute_in_slot,\n",
    "            \"start_offset_s\": start_offset_s,\n",
    "            \"n_points\": npts,\n",
    "            \"n_paths\": n_paths,\n",
    "            \"total_length_m\": slot_length,\n",
    "            \"duration_s\": duration_s,\n",
    "            \"duration_min\": duration_min,\n",
    "            \"avg_speed_kmh\": avg_speed_kmh,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "        })\n",
    "\n",
    "        if WRITE_GEOJSON and n_paths > 0:\n",
    "            # Merge paths by concatenating coordinates while avoiding duplicate joints\n",
    "            merged_coords = []\n",
    "            for pth in slot_paths:\n",
    "                ls = build_linestring_from_path(G, pth)\n",
    "                if ls is None:\n",
    "                    continue\n",
    "                seg = list(ls.coords)\n",
    "                if not merged_coords:\n",
    "                    merged_coords.extend(seg)\n",
    "                else:\n",
    "                    merged_coords.extend(seg[1:])\n",
    "            if len(merged_coords) >= 2:\n",
    "                geoms.append(LineString(merged_coords))\n",
    "                gprops.append({\n",
    "                    \"file\": basename,\n",
    "                    \"date\": date,\n",
    "                    \"slot_id\": slot,\n",
    "                    \"slot_size_min\": M_MINUTES,               # optional, for context\n",
    "                    \"start_minute_in_slot\": start_minute_in_slot,  # optional\n",
    "                    \"n_points\": npts,\n",
    "                    \"n_paths\": n_paths,\n",
    "                    \"total_length_m\": slot_length,\n",
    "                    \"duration_s\": duration_s,\n",
    "                    \"avg_speed_kmh\": avg_speed_kmh\n",
    "                })\n",
    "\n",
    "    # CSV summary\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    out_csv = os.path.join(outdir, f\"{stem}_slots.csv\")\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "    # GeoJSON polylines\n",
    "    if WRITE_GEOJSON and geoms:\n",
    "        gdf = gpd.GeoDataFrame(gprops, geometry=geoms, crs=\"EPSG:4326\")\n",
    "        out_geojson = os.path.join(outdir, f\"{stem}_paths.geojson\")\n",
    "        gdf.to_file(out_geojson, driver=\"GeoJSON\")\n",
    "\n",
    "    print(f\"[OK] {basename} → {os.path.relpath(out_csv, outdir)}\"\n",
    "          f\"{' and paths.geojson' if WRITE_GEOJSON and geoms else ''}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"[1/3] Loading (or caching) road graph for bbox {BBOX} …\")\n",
    "    G = load_graph()\n",
    "\n",
    "    # If you want time-based paths, uncomment the next two lines and set WEIGHT = \"travel_time\"\n",
    "    # G = ox.speed.add_edge_speeds(G)        # adds 'speed_kph'\n",
    "    # G = ox.speed.add_edge_travel_times(G)  # adds 'travel_time'\n",
    "\n",
    "    print(\"[2/3] Processing files…\")\n",
    "    files = sorted(f for f in os.listdir(INPUT_DIR) if f.endswith(\".txt\"))\n",
    "    if not files:\n",
    "        print(\"No .txt files found in INPUT_DIR.\")\n",
    "        return\n",
    "\n",
    "    for fname in tqdm(files):\n",
    "        process_file(G, os.path.join(INPUT_DIR, fname), OUTPUT_DIR)\n",
    "\n",
    "    print(\"[3/3] Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf6e89-9d27-4574-b344-0d737a687c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------------------------------------------------------\n",
    "DESCRIPTION\n",
    "------------------------------------------------------------------------------\n",
    "Purpose\n",
    "-------\n",
    "Post-process slot-wise routed paths (GeoJSON) into a grid-based, time-aware\n",
    "traversal summary. Each reconstructed polyline is raster-walked across a\n",
    "regular grid using a Bresenham-style line traversal, converting distance\n",
    "traversed into cumulative \"reaching time\" per grid cell.\n",
    "\n",
    "Terminology (as used in the paper/artifact)\n",
    "-------------------------------------------\n",
    "• \"Big box\"  := a cellular area (coarse grid cell). It aggregates K×K small boxes.\n",
    "• \"Small box\": a subarea within a cellular area (fine grid cell). Map segments\n",
    "                and their traversal are attributed at this finer granularity.\n",
    "\n",
    "Inputs\n",
    "------\n",
    "• INPUT_DIR: folder containing per-slot routed polylines exported as\n",
    "             \"<stem>_paths.geojson\" (EPSG:4326). Each feature must include:\n",
    "    - geometry            : LineString of the routed path for that slot\n",
    "    - total_length_m      : total routed length (meters) within the slot\n",
    "    - duration_s          : slot traversal duration (seconds)\n",
    "    - n_points, n_paths   : metadata for sanity/QA\n",
    "    - file, date, slot_id : provenance keys\n",
    "    - start_minute_in_slot (optional): minute offset (0..M_MINUTES-1) at which\n",
    "                                       the slot’s first trajectory point occurs.\n",
    "\n",
    "Configuration\n",
    "-------------\n",
    "• BBOX          : geographic bounding box (min_lat, max_lat, min_lon, max_lon).\n",
    "• UTM_CRS       : projected CRS in meters (e.g., UTM zone 50N for Beijing).\n",
    "• CELL_SIZE_X/Y : small-box (subarea) grid resolution in meters.\n",
    "• K             : grouping factor; a big box = K×K small boxes (cellular area).\n",
    "\n",
    "Processing pipeline\n",
    "-------------------\n",
    "1) Project the geographical BBOX to UTM_CRS to obtain metric bounds.\n",
    "2) Construct a regular grid:\n",
    "     - small-grid resolution: CELL_SIZE_X by CELL_SIZE_Y (meters)\n",
    "     - big-grid resolution  : groups of K×K small cells (cellular areas)\n",
    "3) For each LineString in <stem>_paths.geojson:\n",
    "     a) Compute average speed (m/s) = total_length_m / duration_s.\n",
    "     b) Convert the line to a sequence of small-grid indices via Bresenham,\n",
    "        enumerating every crossed small cell along the path.\n",
    "     c) For each Bresenham step, increment cumulative time t_cum by\n",
    "        step_distance / average_speed, yielding the grid \"reaching time\".\n",
    "     d) Emit labels \"BIG_ID/SMALL_ID/REACH_TIME_SECONDS\" in traversal order.\n",
    "     e) Preserve 'start_minute_in_slot' if present (else -1).\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "• OUTPUT_DIR/<stem>_boxes_reach.csv with columns:\n",
    "    - file, date, slot_id\n",
    "    - start_minute_in_slot     : forwarded from input (or -1 if absent)\n",
    "    - n_points, n_paths\n",
    "    - total_length_m\n",
    "    - box_labels               : semicolon-separated sequence of labels\n",
    "                                 \"BIG_ID/SMALL_ID/REACH_TIME_SECONDS\"\n",
    "\n",
    "Notes & Guarantees\n",
    "------------------\n",
    "• \"Big box\" (cellular area) IDs come from (I,J) = (i//K, j//K) where (i,j) are\n",
    "  small-grid indices. \"Small box\" (subarea) IDs index each fine cell uniquely\n",
    "  within the full grid.\n",
    "• Bresenham traversal ensures every intersected small box is visited in a\n",
    "  reproducible order with no gaps.\n",
    "• Reaching time starts at 0 s at the first encountered small box for that\n",
    "  feature and accumulates along the rasterized path.\n",
    "• Assumes inputs were generated by the prior routing pipeline and that GeoJSON\n",
    "  features are valid LineStrings with required properties.\n",
    "------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR  = \"tdrive_routes_speed_MINUTES\"\n",
    "OUTPUT_DIR = \"tdrive_boxes_time_MINUTES\"\n",
    "# Bounding box (lat, lon)\n",
    "BBOX = (39.8, 40.05, 116.2, 116.5)  # (min_lat, max_lat, min_lon, max_lon)\n",
    "# Grid cell size in meters\n",
    "CELL_SIZE_X = 250\n",
    "CELL_SIZE_Y = 250\n",
    "\n",
    "# Grouping factor: how many small cells per big cell\n",
    "K = 4  # (e.g., 4×4 small cells = 1 big cell)\n",
    "\n",
    "# CRS for Beijing (UTM zone 50N, units = meters)\n",
    "UTM_CRS = 32650\n",
    "# ----------------------------------------\n",
    "# CRS for SANFRA (UTM zone 50N, units = meters)\n",
    "#UTM_CRS =  32610\n",
    "\n",
    "def coord_to_grid(x, y, xmin, ymin, dx, dy):\n",
    "    \"\"\"Convert projected (x,y) into grid indices (i,j).\"\"\"\n",
    "    i = int((x - xmin) // dx)\n",
    "    j = int((y - ymin) // dy)\n",
    "    return i, j\n",
    "\n",
    "\n",
    "def bresenham(i0, j0, i1, j1):\n",
    "    \"\"\"Return grid cells between (i0,j0) and (i1,j1) using Bresenham.\"\"\"\n",
    "    cells = []\n",
    "\n",
    "    dx = abs(i1 - i0)\n",
    "    dy = abs(j1 - j0)\n",
    "    sx = 1 if i0 < i1 else -1\n",
    "    sy = 1 if j0 < j1 else -1\n",
    "    err = dx - dy\n",
    "\n",
    "    x, y = i0, j0\n",
    "    while True:\n",
    "        cells.append((x, y))\n",
    "        if x == i1 and y == j1:\n",
    "            break\n",
    "        e2 = 2 * err\n",
    "        if e2 > -dy:\n",
    "            err -= dy\n",
    "            x += sx\n",
    "        if e2 < dx:\n",
    "            err += dx\n",
    "            y += sy\n",
    "    return cells\n",
    "\n",
    "\n",
    "def step_distance(i0, j0, i1, j1, dx, dy):\n",
    "    \"\"\"Distance in meters for one Bresenham step.\"\"\"\n",
    "    if i0 == i1 or j0 == j1:\n",
    "        # horizontal or vertical step\n",
    "        return dx if i0 != i1 else dy\n",
    "    else:\n",
    "        # diagonal step\n",
    "        return math.sqrt(dx**2 + dy**2)\n",
    "\n",
    "\n",
    "def process_geojson(infile, outdir, xmin, ymin, xmax, ymax, dx, dy, nx, ny, k):\n",
    "    \"\"\"\n",
    "    Process one _paths.geojson file with Bresenham grid traversal + reaching time.\n",
    "    Also forwards the 'start_minute_in_slot' property (if present in the GeoJSON)\n",
    "    into the output CSV as a dedicated column.\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(infile).to_crs(epsg=UTM_CRS)\n",
    "\n",
    "    # Number of big cells horizontally and vertically\n",
    "    nbx = (nx + k - 1) // k\n",
    "    nby = (ny + k - 1) // k\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, row in gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "        if geom is None or geom.is_empty or geom.geom_type != \"LineString\":\n",
    "            continue\n",
    "\n",
    "        total_length_m = row[\"total_length_m\"]\n",
    "        duration_s = row.get(\"duration_s\", None)\n",
    "        if duration_s is None or duration_s <= 0 or total_length_m <= 0:\n",
    "            continue\n",
    "\n",
    "        # average speed (m/s) used to convert Bresenham step distances to time\n",
    "        avg_speed_mps = total_length_m / duration_s\n",
    "\n",
    "        # NEW: forward the starting minute within the slot if present in GeoJSON props\n",
    "        # falls back to -1 if not available\n",
    "        start_minute_in_slot = int(row.get(\"start_minute_in_slot\", -1))\n",
    "\n",
    "        coords = list(geom.coords)\n",
    "        timeline = []  # [\"big/small/reaching_time\"]\n",
    "\n",
    "        t_cum = 0.0  # reaching time starts at 0 (seconds)\n",
    "\n",
    "        for m in range(len(coords) - 1):\n",
    "            x0, y0 = coords[m]\n",
    "            x1, y1 = coords[m + 1]\n",
    "            i0, j0 = coord_to_grid(x0, y0, xmin, ymin, dx, dy)\n",
    "            i1, j1 = coord_to_grid(x1, y1, xmin, ymin, dx, dy)\n",
    "            cells = bresenham(i0, j0, i1, j1)\n",
    "\n",
    "            for step in range(len(cells) - 1):\n",
    "                iA, jA = cells[step]\n",
    "                iB, jB = cells[step + 1]\n",
    "\n",
    "                if iA < 0 or jA < 0 or iA >= nx or jA >= ny:\n",
    "                    continue\n",
    "\n",
    "                small_id = jA * nx + iA + 1\n",
    "                I = iA // k\n",
    "                J = jA // k\n",
    "                big_id = J * nbx + I + 1\n",
    "\n",
    "                label = f\"{big_id}/{small_id}/{round(t_cum, 2)}\"\n",
    "                timeline.append(label)\n",
    "\n",
    "                # advance time for next step\n",
    "                d = step_distance(iA, jA, iB, jB, dx, dy)\n",
    "                dt = d / avg_speed_mps\n",
    "                t_cum += dt\n",
    "\n",
    "        # Add last cell at its final reaching time\n",
    "        if timeline:\n",
    "            iL, jL = cells[-1]\n",
    "            if 0 <= iL < nx and 0 <= jL < ny:\n",
    "                small_id = jL * nx + iL + 1\n",
    "                I = iL // k\n",
    "                J = jL // k\n",
    "                big_id = J * nbx + I + 1\n",
    "                label = f\"{big_id}/{small_id}/{round(t_cum, 2)}\"\n",
    "                timeline.append(label)\n",
    "\n",
    "        rows.append({\n",
    "            \"file\": row[\"file\"],\n",
    "            \"date\": row[\"date\"],\n",
    "            \"slot_id\": row[\"slot_id\"],\n",
    "            \"start_minute_in_slot\": start_minute_in_slot,  # <<< NEW COLUMN\n",
    "            \"n_points\": row[\"n_points\"],\n",
    "            \"n_paths\": row[\"n_paths\"],\n",
    "            \"total_length_m\": total_length_m,\n",
    "            \"box_labels\": \";\".join(timeline),\n",
    "        })\n",
    "\n",
    "    # Save CSV\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    basename = os.path.basename(infile).replace(\"_paths.geojson\", \"_boxes_reach.csv\")\n",
    "    outfile = os.path.join(outdir, basename)\n",
    "    pd.DataFrame(rows).to_csv(outfile, index=False)\n",
    "\n",
    "    print(f\"[OK] {os.path.basename(infile)} → {os.path.basename(outfile)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Prepare bbox in UTM\n",
    "    bbox_poly = gpd.GeoDataFrame(\n",
    "        geometry=[LineString([\n",
    "            (BBOX[2], BBOX[0]), (BBOX[3], BBOX[0]),\n",
    "            (BBOX[3], BBOX[1]), (BBOX[2], BBOX[1]),\n",
    "            (BBOX[2], BBOX[0])\n",
    "        ])],\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=UTM_CRS)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = bbox_poly.total_bounds\n",
    "    dx, dy = CELL_SIZE_X, CELL_SIZE_Y\n",
    "\n",
    "    nx = int((xmax - xmin) // dx) + 1\n",
    "    ny = int((ymax - ymin) // dy) + 1\n",
    "    nbx = (nx + K - 1) // K\n",
    "    nby = (ny + K - 1) // K\n",
    "\n",
    "    print(f\"Total small grids: {nx * ny}\")\n",
    "    print(f\"Total big grids: {nbx * nby}\")\n",
    "\n",
    "    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\"_paths.geojson\")]\n",
    "    if not files:\n",
    "        print(\"No _paths.geojson files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(files)} GeoJSON files …\")\n",
    "    for fname in sorted(files):\n",
    "        infile = os.path.join(INPUT_DIR, fname)\n",
    "        process_geojson(infile, OUTPUT_DIR, xmin, ymin, xmax, ymax, dx, dy, nx, ny, K)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc14c7-032a-457a-8bb7-b22b56ec9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Normalized Urgency-Weighted Popularity (UWP) computation across big boxes.\n",
    "\n",
    "Each *_boxes_reach.csv row has:\n",
    "  box_labels = \"big_id/small_id/reaching_time; ...\", where reaching_time is in SECONDS.\n",
    "\n",
    "Let T_final = max(reaching_time) among tokens in the row.\n",
    "We compute weights only for cross-big tokens (skip tokens where token.big_id == start_big)\n",
    "using the standard UWP formulation.\n",
    "\n",
    "Outputs one CSV per starting big box:\n",
    "  big_<BIGID>_popularity_sparse.csv with columns:\n",
    "    day_idx, slot_id, nnz, total_urgency, counts_json\n",
    "\n",
    "Notes:\n",
    "- Extra columns (e.g., start_minute_in_slot) are ignored.\n",
    "- JSON values are rounded floats (ROUND_DECIMALS).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR  = \"tdrive_boxes_time_MINUTES\"            # where *_boxes_reach.csv live\n",
    "OUTPUT_DIR = \"tdrive_UWP_sparse_1hour_250_Normal\"  # output folder\n",
    "FILE_GLOB  = \"*_boxes_reach.csv\"\n",
    "\n",
    "# Column names expected in input CSVs\n",
    "COL_FILE       = \"file\"\n",
    "COL_DATE       = \"date\"\n",
    "COL_SLOT       = \"slot_id\"\n",
    "COL_BOX_LABELS = \"box_labels\"  # \"big/small/time;big/small/time;...\"\n",
    "\n",
    "# ---- UWP calculation parameters ----\n",
    "P             = 3.0        # higher -> more separation near the start\n",
    "CLAMP_01      = True       # clamp weights into [0,1]\n",
    "ROUND_DECIMALS = 6         # rounding for JSON values\n",
    "# ------------------------------------\n",
    "\n",
    "\n",
    "def parse_label_with_time(token: str) -> Optional[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Parse 'big/small/time' -> (big_id:int, small_id:int, t:float)\n",
    "    Returns None if parsing fails or any field is missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = token.strip().split(\"/\")\n",
    "        if len(parts) < 3:\n",
    "            return None\n",
    "        big_id = int(parts[0].strip())\n",
    "        small_id = int(parts[1].strip())\n",
    "        t = float(parts[2].strip())\n",
    "        return big_id, small_id, t\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_day_index_map(all_dates: List[str]) -> dict:\n",
    "    \"\"\"Map unique date strings -> day_idx (1..N) in ascending date order.\"\"\"\n",
    "    unique_sorted = sorted(set(all_dates))\n",
    "    return {d: i + 1 for i, d in enumerate(unique_sorted)}\n",
    "\n",
    "\n",
    "def _clamp01(x: float) -> float:\n",
    "    if CLAMP_01:\n",
    "        if x < 0.0:\n",
    "            return 0.0\n",
    "        if x > 1.0:\n",
    "            return 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def uwp_weights(times_rel: List[float]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Given a list of relative times t/T_final in [0,1], return UWP weights.\n",
    "    \"\"\"\n",
    "    if not times_rel:\n",
    "        return []\n",
    "    return [_clamp01((1.0 - tr) ** P) for tr in times_rel]\n",
    "\n",
    "\n",
    "def process_folder(input_dir: str, output_dir: str) -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Load all *_boxes_reach.csv\n",
    "    paths = sorted(glob.glob(os.path.join(input_dir, FILE_GLOB)))\n",
    "    if not paths:\n",
    "        print(\"No *_boxes_reach.csv files found.\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p, dtype={COL_FILE: \"string\", COL_DATE: \"string\"}, keep_default_na=False)\n",
    "            # Keep only the columns we need; extra columns (e.g., start_minute_in_slot) are ignored\n",
    "            cols = [COL_FILE, COL_DATE, COL_SLOT, COL_BOX_LABELS]\n",
    "            df = df[[c for c in cols if c in df.columns]].copy()\n",
    "            frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {os.path.basename(p)}: {e}\")\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No valid data after reading files.\")\n",
    "        return\n",
    "\n",
    "    data = pd.concat(frames, ignore_index=True)\n",
    "    if data.empty:\n",
    "        print(\"Input data is empty.\")\n",
    "        return\n",
    "\n",
    "    # 2) Build day indices\n",
    "    day_map = build_day_index_map(data[COL_DATE].tolist())\n",
    "    data[\"day_idx\"] = data[COL_DATE].map(day_map)\n",
    "\n",
    "    # 3) Accumulator:\n",
    "    #    uwp[start_big][(day_idx, slot_id)] = Counter({small_id: weight_sum})\n",
    "    uwp = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "    # 4) Iterate rows\n",
    "    for _, row in data.iterrows():\n",
    "        date_str   = row.get(COL_DATE, \"\")\n",
    "        slot_val   = row.get(COL_SLOT, None)\n",
    "        labels_str = row.get(COL_BOX_LABELS, \"\")\n",
    "\n",
    "        if not date_str or slot_val is None or not labels_str:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            slot_id = int(slot_val)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        tokens = [tok.strip() for tok in labels_str.split(\";\") if tok.strip()]\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        # Parse all tokens with time\n",
    "        parsed = [parse_label_with_time(tok) for tok in tokens]\n",
    "        parsed = [x for x in parsed if x is not None]\n",
    "        if not parsed:\n",
    "            continue\n",
    "\n",
    "        # Determine starting big box from the FIRST token\n",
    "        start_big = parsed[0][0]\n",
    "\n",
    "        # Destination time = maximum reaching time among ALL tokens in this row\n",
    "        T_final = max(t for (_, _, t) in parsed)\n",
    "        if T_final <= 0:\n",
    "            continue\n",
    "\n",
    "        # Consider only cross-big tokens for weighting\n",
    "        cross = [(big, small, t) for (big, small, t) in parsed if big != start_big]\n",
    "        if not cross:\n",
    "            continue\n",
    "\n",
    "        # Relative times for selected tokens\n",
    "        times_rel = [t / T_final for (_, _, t) in cross]\n",
    "\n",
    "        # Compute UWP weights\n",
    "        w_list = uwp_weights(times_rel)\n",
    "\n",
    "        day_idx = day_map[date_str]\n",
    "        ctr = uwp[start_big][(day_idx, slot_id)]\n",
    "\n",
    "        # Accumulate weights per small_id\n",
    "        for ((_, small_id, _), w) in zip(cross, w_list):\n",
    "            ctr[int(small_id)] += float(w)\n",
    "\n",
    "    # 5) Write one CSV per starting big box\n",
    "    for start_big, bucket in uwp.items():\n",
    "        out_path = os.path.join(output_dir, f\"big_{start_big}_popularity_sparse.csv\")\n",
    "        with open(out_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Header: day_idx, slot_id, nnz (#unique small boxes), total_urgency, counts_json\n",
    "            writer.writerow([\"day_idx\", \"slot_id\", \"nnz\", \"total_urgency\", \"counts_json\"])\n",
    "            for (day_idx, slot_id), ctr in sorted(bucket.items()):\n",
    "                nnz = len(ctr)\n",
    "                total_urgency = float(sum(ctr.values()))\n",
    "                counts_json = json.dumps(\n",
    "                    {str(k): round(float(v), ROUND_DECIMALS) for k, v in sorted(ctr.items())},\n",
    "                    separators=(',', ':'), sort_keys=True\n",
    "                )\n",
    "                writer.writerow([day_idx, slot_id, nnz, round(total_urgency, ROUND_DECIMALS), counts_json])\n",
    "\n",
    "        print(f\"[OK] Wrote {os.path.basename(out_path)} with {len(bucket)} rows (UWP computed)\")\n",
    "\n",
    "    # Also write day index map for reference\n",
    "    pd.DataFrame(\n",
    "        [{\"date\": d, \"day_idx\": idx} for d, idx in sorted(day_map.items(), key=lambda x: x[1])]\n",
    "    ).to_csv(os.path.join(output_dir, \"day_index_map.csv\"), index=False)\n",
    "    print(\"[OK] Wrote day_index_map.csv\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    process_folder(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d08c8b-2602-46b9-982b-ba25a1217b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chosing the cellular areas (big_boxes) for the experimemts.\n",
    "Enumerate small_ids across selected big boxes, align rows/columns, and write:\n",
    "  - global_small_ids.csv        : col_idx (0-based), small_id\n",
    "  - reverse_small_ids.csv       : small_id, col_idx (0-based)\n",
    "  - big_<ID>_vectors.csv        : day_idx, slot_id, vector (zero-filled, aligned)\n",
    "\n",
    "Behavior:\n",
    "- Columns = UNION of small_ids observed in the SELECTED files only (not the whole folder).\n",
    "- Rows    = UNION of (day_idx, slot_id) across the SELECTED files; zero-fill where missing.\n",
    "- Values  = parsed from counts_json as float (works for both popularity ints and UWP floats).\n",
    "\n",
    "Adjust SPARSE_DIR / OUT_DIR / BIG_IDS as needed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "SPARSE_DIR = \"tdrive_UWP_sparse_1hour_250_Normal\"  # or popularity folder\n",
    "OUT_DIR    = \"UWP_ENUM_Minutes\"    # output folder\n",
    "FILE_GLOB  = \"big_*_popularity_sparse.csv\"\n",
    "\n",
    "# Select ONLY these big boxes; global small_ids are built from them\n",
    "BIG_IDS = [21, 22]   # <-- edit as needed Change according to need, if want to reproduce result for Tdrive then the is [21,22], \n",
    "#for San Francisco [12,13]\n",
    "# Vector serialization (keep float to support UWP). If using pure ints, it's fine too.\n",
    "ROUND_DECIMALS = 6\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def safe_load_json(s: str):\n",
    "    try:\n",
    "        return json.loads(s) if isinstance(s, str) and s else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def list_available_files(sparse_dir: str, pattern: str):\n",
    "    \"\"\"Return {big_id: filepath} for all matching files.\"\"\"\n",
    "    out = {}\n",
    "    for p in glob.glob(os.path.join(sparse_dir, pattern)):\n",
    "        base = os.path.basename(p)\n",
    "        if base.startswith(\"big_\") and base.endswith(\"_popularity_sparse.csv\"):\n",
    "            core = base[len(\"big_\"):-len(\"_popularity_sparse.csv\")]\n",
    "            try:\n",
    "                out[int(core)] = p\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def collect_union_small_ids(files_map):\n",
    "    \"\"\"Union of small_ids across the selected files, returned as a sorted list.\"\"\"\n",
    "    sids = set()\n",
    "    for _, path in sorted(files_map.items()):\n",
    "        try:\n",
    "            df = pd.read_csv(path, usecols=[\"counts_json\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "        for _, r in df.iterrows():\n",
    "            d = safe_load_json(r.get(\"counts_json\", \"\"))\n",
    "            for k in d.keys():\n",
    "                try:\n",
    "                    sids.add(int(k))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return sorted(sids)\n",
    "\n",
    "\n",
    "def collect_union_day_slot(files_map):\n",
    "    \"\"\"Union of (day_idx, slot_id) across selected files, returned as a sorted DataFrame.\"\"\"\n",
    "    pairs = set()\n",
    "    for _, path in files_map.items():\n",
    "        try:\n",
    "            df = pd.read_csv(path, usecols=[\"day_idx\", \"slot_id\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "        for _, r in df.iterrows():\n",
    "            d = r.get(\"day_idx\")\n",
    "            s = r.get(\"slot_id\")\n",
    "            if pd.notna(d) and pd.notna(s):\n",
    "                try:\n",
    "                    pairs.add((int(d), int(s)))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not pairs:\n",
    "        return pd.DataFrame(columns=[\"day_idx\", \"slot_id\"])\n",
    "    return pd.DataFrame(sorted(pairs), columns=[\"day_idx\", \"slot_id\"])\n",
    "\n",
    "\n",
    "def write_global_index(out_dir, small_ids_sorted):\n",
    "    \"\"\"Write the enumeration mapping: col_idx (0-based), small_id.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    path = os.path.join(out_dir, \"global_small_ids.csv\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"col_idx\", \"small_id\"])\n",
    "        for i, sid in enumerate(small_ids_sorted):  # 0-based\n",
    "            w.writerow([i, sid])\n",
    "    print(f\"[OK] Wrote global_small_ids.csv (K={len(small_ids_sorted)})\")\n",
    "\n",
    "\n",
    "def write_reverse_mapping(out_dir, small_ids_sorted):\n",
    "    \"\"\"Write reverse mapping: small_id -> col_idx (0-based).\"\"\"\n",
    "    path = os.path.join(out_dir, \"reverse_small_ids.csv\")\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"small_id\", \"col_idx\"])\n",
    "        for i, sid in enumerate(small_ids_sorted):  # 0-based\n",
    "            w.writerow([sid, i])\n",
    "    print(f\"[OK] Wrote reverse_small_ids.csv (K={len(small_ids_sorted)})\")\n",
    "\n",
    "\n",
    "def vector_to_string(vec):\n",
    "    return \"[\" + \",\".join(f\"{v:.{ROUND_DECIMALS}f}\" for v in vec) + \"]\"\n",
    "\n",
    "\n",
    "def load_sparse_map(path):\n",
    "    \"\"\"\n",
    "    Load one big_* file into:\n",
    "      {(day_idx, slot_id): {small_id: float_value, ...}}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return out\n",
    "\n",
    "    if not {\"day_idx\", \"slot_id\", \"counts_json\"}.issubset(df.columns):\n",
    "        return out\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        d = r.get(\"day_idx\")\n",
    "        s = r.get(\"slot_id\")\n",
    "        if pd.isna(d) or pd.isna(s):\n",
    "            continue\n",
    "        try:\n",
    "            key = (int(d), int(s))\n",
    "        except Exception:\n",
    "            continue\n",
    "        dd = {}\n",
    "        counts = safe_load_json(r.get(\"counts_json\", \"\"))\n",
    "        for k, v in counts.items():\n",
    "            try:\n",
    "                dd[int(k)] = float(v)  # supports UWP floats & int popularity\n",
    "            except Exception:\n",
    "                continue\n",
    "        out[key] = dd\n",
    "    return out\n",
    "\n",
    "\n",
    "def convert_to_vectors(files_map, small_ids_sorted, union_day_slot, out_dir):\n",
    "    \"\"\"Write one vectors CSV per selected big box, zero-filling missing.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    col_index = {sid: i for i, sid in enumerate(small_ids_sorted)}\n",
    "    K = len(small_ids_sorted)\n",
    "\n",
    "    for bid, path in sorted(files_map.items()):\n",
    "        sparse_map = load_sparse_map(path)\n",
    "        rows = []\n",
    "\n",
    "        for _, rs in union_day_slot.iterrows():\n",
    "            d = int(rs[\"day_idx\"])\n",
    "            s = int(rs[\"slot_id\"])\n",
    "            dd = sparse_map.get((d, s), {})  # {} => zero vector\n",
    "\n",
    "            vec = [0.0] * K\n",
    "            for sid, val in dd.items():\n",
    "                j = col_index.get(sid)\n",
    "                if j is not None:\n",
    "                    vec[j] = val\n",
    "\n",
    "            rows.append({\"day_idx\": d, \"slot_id\": s, \"vector\": vector_to_string(vec)})\n",
    "\n",
    "        out_df = pd.DataFrame(rows).sort_values([\"day_idx\", \"slot_id\"])\n",
    "        out_name = f\"big_{bid}_vectors.csv\"\n",
    "        out_path = os.path.join(out_dir, out_name)\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"[OK] Wrote {out_name}  rows={len(out_df)}  vec_dim={K}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Keep ONLY the selected big boxes\n",
    "    all_files = list_available_files(SPARSE_DIR, FILE_GLOB)\n",
    "    files_map = {bid: all_files[bid] for bid in BIG_IDS if bid in all_files}\n",
    "    missing = [bid for bid in BIG_IDS if bid not in files_map]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Missing big IDs: {missing}\")\n",
    "    if not files_map:\n",
    "        print(\"ERROR: none of the requested big IDs exist in the folder.\")\n",
    "        return\n",
    "\n",
    "    # 1) UNION of small_ids across selected files -> enumerate 0..K-1\n",
    "    small_ids_sorted = collect_union_small_ids(files_map)\n",
    "    write_global_index(OUT_DIR, small_ids_sorted)\n",
    "    write_reverse_mapping(OUT_DIR, small_ids_sorted)\n",
    "\n",
    "    # 2) UNION of (day_idx, slot_id) across selected files -> consistent rows\n",
    "    union_day_slot = collect_union_day_slot(files_map)\n",
    "    if union_day_slot.empty:\n",
    "        print(\"[INFO] No (day_idx, slot_id) pairs found. Nothing to write.\")\n",
    "        return\n",
    "\n",
    "    # 3) Convert each selected file -> aligned, zero-filled vectors\n",
    "    convert_to_vectors(files_map, small_ids_sorted, union_day_slot, OUT_DIR)\n",
    "\n",
    "    print(f\"All done. Outputs at: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57282862-255e-4c9b-a6e8-aa17a9363e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR = \"UWP_ENUM_Minutes\"      # Folder where your CSVs are\n",
    "FILES = [\"big_21_vectors.csv\", \"big_22_vectors.csv\"]  # Add more if needed\n",
    "# <-- edit as needed Change according to need, if want to reproduce result for Tdrive then the is [21,22], \n",
    "#for San Francisco [12,13]\n",
    "SAVE_PATH = \"big_UWP_tensor.npy\"        # Save in current directory\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def parse_vector(vec_str):\n",
    "    \"\"\"Convert '[0,1,2,...]' string into list[float].\"\"\"\n",
    "    try:\n",
    "        return [float(x) for x in ast.literal_eval(vec_str)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "tensors = []\n",
    "\n",
    "for fname in FILES:\n",
    "    path = os.path.join(INPUT_DIR, fname)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] File not found: {path}\")\n",
    "        continue\n",
    "\n",
    "    # Read CSV and parse vector column\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"vector_list\"] = df[\"vector\"].apply(parse_vector)\n",
    "\n",
    "    # Convert to NumPy array (num_timeslots, vector_len)\n",
    "    arr = np.array(df[\"vector_list\"].to_list())\n",
    "    tensors.append(arr)\n",
    "    print(f\"[OK] Loaded {fname} → shape {arr.shape}\")\n",
    "\n",
    "if not tensors:\n",
    "    raise RuntimeError(\"No valid CSV files loaded. Exiting.\")\n",
    "\n",
    "# Stack all arrays into one 3D tensor: (num_big_boxes, num_timeslots, vector_len)\n",
    "tensor = np.stack(tensors, axis=0)\n",
    "print(f\"\\n✅ Final tensor shape: {tensor.shape}\")\n",
    "\n",
    "# Save tensor in the **current directory** as `big_tensor.npy`\n",
    "np.save(SAVE_PATH, tensor)\n",
    "print(f\"[OK] Saved combined tensor → {os.path.abspath(SAVE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d595ca-69d9-49a6-90e2-6efe2fbd5f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code produces, for each cellular area (“big box”) and time slot, the list of\n",
    "HD map segments (small boxes) requested by vehicles together with their Urgency-Weighted Popularity (UWP) weights.\n",
    "\n",
    "INPUT_DIR contains many *_boxes_reach.csv files with columns:\n",
    "  - file (original taxi filename; vehicle id derived from its stem)\n",
    "  - date (YYYY-MM-DD)\n",
    "  - slot_id (integer hour slot)\n",
    "  - start_minute_in_slot (optional; integer minute within slot)\n",
    "  - box_labels: \"big_id/small_id/reaching_time;big_id/small_id/reaching_time;...\"\n",
    "\n",
    "For each row:\n",
    "  - Determine starting big box (start_big) from the FIRST token's big_id.\n",
    "  - Compute T_final = max(reaching_time) across tokens in the row.\n",
    "  - Build a sublist: [vehicle_id, start_minute, [[small_id, uwp_weight], ...]],\n",
    "    where uwp_weight is computed from relative time t/T_final via the UWP formula.\n",
    "  - Only include tokens whose big_id != start_big (skip within-same-big-box visits).\n",
    "\n",
    "We aggregate by (start_big, day_idx, slot_id).\n",
    "For each (day_idx, slot_id), we collect a list of those sublists (one per input row).\n",
    "\n",
    "OUTPUT:\n",
    "  OUTPUT_DIR / big_<BIGID>.csv\n",
    "    Columns:\n",
    "      - day_idx\n",
    "      - slot_id\n",
    "      - entries_json   (JSON array of sublists as described above)\n",
    "\n",
    "Also writes:\n",
    "  OUTPUT_DIR / day_index_map.csv  (date -> day_idx mapping)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR  = \"tdrive_boxes_time_MINUTES\"\n",
    "OUTPUT_DIR = \"bigbox_slot_dicts_uwp\"\n",
    "FILE_GLOB  = \"*_boxes_reach.csv\"\n",
    "\n",
    "# UWP parameters\n",
    "P         = 3.0     # controls separation (higher → sharper early emphasis)\n",
    "CLAMP_01  = True    # clamp weights to [0,1]\n",
    "ROUND_DEC = 6       # rounding for weights inside JSON\n",
    "\n",
    "# Column names in *_boxes_reach.csv\n",
    "COL_FILE        = \"file\"\n",
    "COL_DATE        = \"date\"\n",
    "COL_SLOT        = \"slot_id\"\n",
    "COL_BOX_LABELS  = \"box_labels\"\n",
    "COL_START_MIN   = \"start_minute_in_slot\"  # optional\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def parse_label_with_time(token: str) -> Optional[Tuple[int, int, float]]:\n",
    "    \"\"\"Parse 'big/small/time' -> (big_id:int, small_id:int, t:float).\"\"\"\n",
    "    try:\n",
    "        parts = token.strip().split(\"/\")\n",
    "        if len(parts) < 3:\n",
    "            return None\n",
    "        big_id = int(parts[0].strip())\n",
    "        small_id = int(parts[1].strip())\n",
    "        t = float(parts[2].strip())\n",
    "        return big_id, small_id, t\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_day_index_map(all_dates: List[str]) -> dict:\n",
    "    \"\"\"Map unique date strings -> day_idx (1..N) in ascending date order.\"\"\"\n",
    "    uniq = sorted(set(all_dates))\n",
    "    return {d: i + 1 for i, d in enumerate(uniq)}\n",
    "\n",
    "\n",
    "def stem_from_path(path: str) -> str:\n",
    "    \"\"\"Return basename without extension.\"\"\"\n",
    "    base = os.path.basename(path)\n",
    "    if \".\" in base:\n",
    "        return \".\".join(base.split(\".\")[:-1]) or base\n",
    "    return base\n",
    "\n",
    "\n",
    "def extract_vehicle_id(file_field: str) -> str:\n",
    "    \"\"\"Derive vehicle id from the 'file' column by taking the stem.\"\"\"\n",
    "    return stem_from_path(file_field)\n",
    "\n",
    "\n",
    "def _clamp01(x: float) -> float:\n",
    "    if CLAMP_01:\n",
    "        if x < 0.0:\n",
    "            return 0.0\n",
    "        if x > 1.0:\n",
    "            return 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def uwp_weights(times_rel: List[float]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Given relative times t/T in [0,1] for cross-big tokens of a row,\n",
    "    return per-token UWP weights: w = (1 - t/T)^P.\n",
    "    \"\"\"\n",
    "    if not times_rel:\n",
    "        return []\n",
    "    return [_clamp01((1.0 - tr) ** P) for tr in times_rel]\n",
    "\n",
    "\n",
    "def process(input_dir: str, output_dir: str) -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Load all *_boxes_reach.csv\n",
    "    paths = sorted(glob.glob(os.path.join(input_dir, FILE_GLOB)))\n",
    "    if not paths:\n",
    "        print(\"No *_boxes_reach.csv files found.\")\n",
    "        return\n",
    "\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                p,\n",
    "                dtype={COL_FILE: \"string\", COL_DATE: \"string\"},\n",
    "                keep_default_na=False,\n",
    "            )\n",
    "            # Keep only relevant columns; tolerate missing start_minute_in_slot\n",
    "            cols = [COL_FILE, COL_DATE, COL_SLOT, COL_BOX_LABELS, COL_START_MIN]\n",
    "            df = df[[c for c in cols if c in df.columns]].copy()\n",
    "            frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {os.path.basename(p)}: {e}\")\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No valid data after reading files.\")\n",
    "        return\n",
    "\n",
    "    data = pd.concat(frames, ignore_index=True)\n",
    "    if data.empty:\n",
    "        print(\"Input data is empty.\")\n",
    "        return\n",
    "\n",
    "    # 2) day_idx mapping\n",
    "    day_map = build_day_index_map(data[COL_DATE].tolist())\n",
    "    data[\"day_idx\"] = data[COL_DATE].map(day_map)\n",
    "\n",
    "    has_start_min_col = (COL_START_MIN in data.columns)\n",
    "\n",
    "    # 3) Accumulator:\n",
    "    # agg[start_big][(day_idx, slot_id)] = list of sublists\n",
    "    # sublist = [vehicle_id, start_minute, [[small_id, uwp_weight], ...]]\n",
    "    agg = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # 4) Iterate rows\n",
    "    for _, row in data.iterrows():\n",
    "        date_str   = row.get(COL_DATE, \"\")\n",
    "        slot_val   = row.get(COL_SLOT, None)\n",
    "        labels_str = row.get(COL_BOX_LABELS, \"\")\n",
    "        file_val   = row.get(COL_FILE, \"\")\n",
    "\n",
    "        if not date_str or slot_val is None or not labels_str or not file_val:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            slot_id = int(slot_val)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        tokens = [tok.strip() for tok in str(labels_str).split(\";\") if tok.strip()]\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        parsed = [parse_label_with_time(tok) for tok in tokens]\n",
    "        parsed = [x for x in parsed if x is not None]\n",
    "        if not parsed:\n",
    "            continue\n",
    "\n",
    "        # Start big from FIRST token\n",
    "        start_big = parsed[0][0]\n",
    "\n",
    "        # Destination time = max reaching time among tokens\n",
    "        T_final = max(t for (_, _, t) in parsed)\n",
    "        # If no time progression, we end up with empty weights for this row\n",
    "        if T_final <= 0:\n",
    "            uwp_pairs: List[List[float]] = []\n",
    "        else:\n",
    "            # Keep only cross-big visits\n",
    "            cross = [(bid, sid, t) for (bid, sid, t) in parsed if bid != start_big]\n",
    "            if not cross:\n",
    "                uwp_pairs = []\n",
    "            else:\n",
    "                times_rel = [t / T_final for (_, _, t) in cross]\n",
    "                w_list = uwp_weights(times_rel)\n",
    "                uwp_pairs = [[int(sid), round(float(w), ROUND_DEC)] for ((_, sid, _), w) in zip(cross, w_list)]\n",
    "\n",
    "        # minute\n",
    "        if has_start_min_col:\n",
    "            try:\n",
    "                m = row.get(COL_START_MIN, -1)\n",
    "                start_min = int(m) if pd.notna(m) else -1\n",
    "            except Exception:\n",
    "                start_min = -1\n",
    "        else:\n",
    "            start_min = -1\n",
    "\n",
    "        # vehicle id\n",
    "        vehicle_id = extract_vehicle_id(str(file_val))\n",
    "\n",
    "        # sublist for this row\n",
    "        sublist = [vehicle_id, start_min, uwp_pairs]\n",
    "\n",
    "        day_idx = int(row[\"day_idx\"])\n",
    "        agg[start_big][(day_idx, slot_id)].append(sublist)\n",
    "\n",
    "    # 5) Write one CSV per start_big\n",
    "    for start_big, bucket in agg.items():\n",
    "        out_path = os.path.join(output_dir, f\"big_{start_big}.csv\")\n",
    "        with open(out_path, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"day_idx\", \"slot_id\", \"entries_json\"])\n",
    "            for (day_idx, slot_id) in sorted(bucket.keys()):\n",
    "                entries = bucket[(day_idx, slot_id)]  # list of sublists\n",
    "                entries_json = json.dumps(entries, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "                w.writerow([day_idx, slot_id, entries_json])\n",
    "        print(f\"[OK] Wrote {os.path.basename(out_path)} with {len(bucket)} keys\")\n",
    "\n",
    "    # Also write reference day index map\n",
    "    day_map_path = os.path.join(output_dir, \"day_index_map.csv\")\n",
    "    pd.DataFrame(\n",
    "        [{\"date\": d, \"day_idx\": idx} for d, idx in sorted(day_map.items(), key=lambda x: x[1])]\n",
    "    ).to_csv(day_map_path, index=False)\n",
    "    print(f\"[OK] Wrote day_index_map.csv at {day_map_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    process(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb384db-d03b-47d0-83cc-a7de573e28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a nested Python dictionary from multiple big_<ID>.csv files, the cellular areas which are considered together for experiments from\n",
    "all the cellular area in the city.\n",
    "\n",
    "INPUT:\n",
    "  - BIGBOX_IDS: list of big box IDs to load\n",
    "  - Each file: OUTPUT_DIR / big_<ID>.csv\n",
    "      Columns: day_idx, slot_id, entries_json\n",
    "      entries_json = list of sublists: [vehicle_id, start_minute, [[small_id, uwp_weight], ...]]\n",
    "\n",
    "OUTPUT:\n",
    "  - A Python dictionary with structure:\n",
    "        result = {\n",
    "            big_id_1: {\n",
    "                (day_idx, slot_id): [\n",
    "                    [vehicle_id, start_minute, [[small_id, uwp_weight], ...]],\n",
    "                    ...\n",
    "                ],\n",
    "                ...\n",
    "            },\n",
    "            big_id_2: {\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INPUT_DIR = \"bigbox_slot_dicts_uwp\"  # folder with big_<ID>.csv\n",
    "BIGBOX_IDS = [21,22]   # For Cellular area 21,22\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def load_bigbox_dict(input_dir: str, big_ids: list) -> dict:\n",
    "    \"\"\"\n",
    "    Load multiple big_<ID>.csv files into a nested dictionary.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for bid in big_ids:\n",
    "        path = os.path.join(input_dir, f\"big_{bid}.csv\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[WARN] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path, dtype={\"day_idx\": int, \"slot_id\": int})\n",
    "        big_dict = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            day_idx = int(row[\"day_idx\"])\n",
    "            slot_id = int(row[\"slot_id\"])\n",
    "            key = (day_idx, slot_id)\n",
    "\n",
    "            try:\n",
    "                entries = json.loads(row[\"entries_json\"])\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"[ERROR] JSON parse failed for big {bid}, day {day_idx}, slot {slot_id}\")\n",
    "                entries = []\n",
    "\n",
    "            big_dict[key] = entries\n",
    "\n",
    "        result[bid] = big_dict\n",
    "        print(f\"[OK] Loaded big_{bid}.csv → {len(big_dict)} time slots\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "bigbox_data = load_bigbox_dict(INPUT_DIR, BIGBOX_IDS)\n",
    "\n",
    "# ✅ Example usage: iterate over dictionary\n",
    "for bid, slots in bigbox_data.items():\n",
    "    print(f\"\\n=== BIG BOX {bid} ===\")\n",
    "    for (day_idx, slot_id), entries in list(slots.items())[:3]:  # show first 3\n",
    "        print(f\"Day {day_idx}, Slot {slot_id}: {len(entries)} vehicle records\")\n",
    "        for vehicle_entry in entries[:2]:  # show first 2 vehicles\n",
    "            print(\"   \", vehicle_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0433bb4-ab44-408b-b82e-0481a8ce1f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "def load_reverse_map(csv_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    reverse_small_ids.csv -> {small_id: col_idx}\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, dtype={\"small_id\": int, \"col_idx\": int})\n",
    "    return dict(zip(df[\"small_id\"], df[\"col_idx\"]))\n",
    "\n",
    "def map_small_ids_to_global(bigbox_data: dict,\n",
    "                            reverse_map_csv: str,\n",
    "                            skip_unmapped: bool = True,\n",
    "                            inplace: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Replace small_id with global col_idx using reverse_small_ids.csv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bigbox_data : dict\n",
    "        { big_id: { (day_idx, slot_id): [ [vehicle_id, start_minute, [[small_id, weight], ...]], ... ] } }\n",
    "    reverse_map_csv : str\n",
    "        Path to CSV with columns: small_id,col_idx\n",
    "    skip_unmapped : bool\n",
    "        If True, drops pairs whose small_id isn't in the reverse map.\n",
    "        If False, keeps original small_id for those pairs.\n",
    "    inplace : bool\n",
    "        If True, modify bigbox_data in place. Otherwise, return a deep-copied mapped dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Same structure as input, with small_ids replaced by col_idx where mapped.\n",
    "    \"\"\"\n",
    "    rev_map = load_reverse_map(reverse_map_csv)\n",
    "\n",
    "    target = bigbox_data if inplace else copy.deepcopy(bigbox_data)\n",
    "\n",
    "    missing = 0\n",
    "    total   = 0\n",
    "\n",
    "    for big_id, slot_dict in target.items():\n",
    "        for key, vehicle_list in slot_dict.items():\n",
    "            # vehicle_list: [ [vehicle_id, start_minute, [[small_id, weight], ...]], ... ]\n",
    "            for entry in vehicle_list:\n",
    "                # entry: [vehicle_id, start_minute, pairs]\n",
    "                pairs = entry[2]\n",
    "                new_pairs = []\n",
    "                for sid, w in pairs:\n",
    "                    total += 1\n",
    "                    sid_int = int(sid)\n",
    "                    if sid_int in rev_map:\n",
    "                        new_pairs.append([rev_map[sid_int], float(w)])\n",
    "                    else:\n",
    "                        missing += 1\n",
    "                        if not skip_unmapped:\n",
    "                            new_pairs.append([sid_int, float(w)])\n",
    "                        # else: drop it\n",
    "                entry[2] = new_pairs  # replace pairs list\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[INFO] Mapping done. {missing}/{total} pairs had no mapping \"\n",
    "              f\"({missing/total:.2%}). {'Dropped' if skip_unmapped else 'Kept'} unmapped.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Mapping done. All {total} pairs mapped.\")\n",
    "\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15642fa9-0d40-4476-ae5a-2e4931de66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the reverse map CSV\n",
    "reverse_map_csv = \"UWP_ENUM_Minutes/reverse_small_ids.csv\"\n",
    "\n",
    "# Create a new mapped dictionary (does NOT modify the original)\n",
    "mapped_data = map_small_ids_to_global(\n",
    "    bigbox_data, \n",
    "    reverse_map_csv,\n",
    "    skip_unmapped=True,   # drop small IDs not found in the reverse map\n",
    "    inplace=False         # set to True if you want to overwrite bigbox_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f68d3-14a8-4f8e-ac8b-ad9beb21ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "car4=[]\n",
    "car4.append(mapped_data)\n",
    "np.save('car4real.npy',car4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357db2e6-8a2e-48c7-b376-143e3b609649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
